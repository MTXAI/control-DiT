{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-03T15:33:30.726661Z",
     "start_time": "2024-06-03T15:33:30.703473Z"
    }
   },
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder, ImageNet\n",
    "\n",
    "from utils.dataset import CustomDataset\n",
    "from utils.image_tools import ImageTools\n",
    "\n",
    "# CUDA = True if torch.cuda.is_available() else False\n",
    "CUDA = False\n",
    "Tensor = torch.cuda.FloatTensor if CUDA else torch.FloatTensor\n",
    "Device = \"cuda\" if CUDA else \"cpu\"\n",
    "imgTools = ImageTools()\n",
    "\n",
    "root = \"../datasets/imagenet2012_processed_train\"\n",
    "batch_size = 1\n",
    "num_workers = 0\n",
    "image_size = 256\n",
    "\n",
    "features_dir = os.path.join(root, f'imagenet{image_size}_features')\n",
    "labels_dir = os.path.join(root, f'imagenet{image_size}_labels')\n",
    "conditions_dir = os.path.join(root, f'imagenet{image_size}_conditions')\n",
    "\n",
    "# Setup data:\n",
    "dataset = CustomDataset(features_dir=features_dir, labels_dir=labels_dir, conditions_dir=conditions_dir)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    idx = 0\n",
    "    x, y, z = None, None, None\n",
    "    for x, y, z in loader:\n",
    "        idx += 1\n",
    "        if idx < 11:\n",
    "            continue\n",
    "        x = x.to(Device)\n",
    "        y = y.to(Device)\n",
    "        z = z.to(Device)\n",
    "        x = x.squeeze(dim=1)\n",
    "        y = y.squeeze(dim=1)\n",
    "        z = z.squeeze(dim=1)\n",
    "        if idx > 0:\n",
    "            break\n",
    "\n",
    "    print(idx, x.shape, y.shape, z.shape)\n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "x, y, z = main()\n",
    "print(x.shape, z.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 torch.Size([1, 3, 256, 256]) torch.Size([1]) torch.Size([1, 256, 256])\n",
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T15:33:39.793731Z",
     "start_time": "2024-06-03T15:33:30.727913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from diffusers import AutoencoderKL\n",
    "from copy import deepcopy\n",
    "from models.DiT import DiT_models\n",
    "from models.control_DiT import ControlDiT_models\n",
    "from diffusion import create_diffusion\n",
    "\n",
    "\n",
    "def requires_grad(model, flag=True):\n",
    "    \"\"\"\n",
    "    Set requires_grad flag for all parameters in a model.\n",
    "    \"\"\"\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "\n",
    "# Load model:\n",
    "assert image_size % 8 == 0, \"Image size must be divisible by 8 (for the VAE encoder).\"\n",
    "latent_size = image_size // 8\n",
    "model_type = 'DiT-XL/2'\n",
    "dit_model = DiT_models[model_type](\n",
    "    input_size=latent_size,\n",
    "    num_classes=1000,\n",
    ").to(Device)\n",
    "\n",
    "\n",
    "def find_model(model_name):\n",
    "    assert os.path.isfile(model_name), f'Could not find DiT checkpoint at {model_name}'\n",
    "    checkpoint = torch.load(model_name, map_location=lambda storage, loc: storage)\n",
    "    if \"ema\" in checkpoint:  # supports checkpoints from train.py\n",
    "        checkpoint = checkpoint[\"ema\"]\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "# ckpt_path = \"../pretrained_models/DiT-XL-2-256x256.pt\"\n",
    "# state_dict = find_model(ckpt_path)\n",
    "# dit_model.load_state_dict(state_dict)\n",
    "# dit_model.learn_sigma = False\n",
    "dit_model.eval()\n",
    "# model.eval()  # important!\n",
    "# Create model:\n",
    "model = ControlDiT_models[model_type](\n",
    "    dit=dit_model,\n",
    "    input_size=latent_size,\n",
    "    num_classes=1000,\n",
    "    learn_sigma=False,\n",
    ").to(Device)\n",
    "model.train()\n",
    "# # Note that parameter initialization is done within the DiT constructor\n",
    "# model = model.to(Device)\n",
    "# ema = deepcopy(model).to(Device)  # Create an EMA of the model for use after training\n",
    "# requires_grad(ema, False)\n",
    "diffusion = create_diffusion(timestep_respacing=\"\")  # default: 1000 steps, linear noise schedule\n",
    "# vae = AutoencoderKL.from_pretrained(f\"../pretrained_models/sd-vae-ft-ema\").to(Device)\n",
    "vae = AutoencoderKL.from_pretrained(f\"stabilityai/sd-vae-ft-ema\").to(Device)\n",
    "# Setup optimizer (we used default Adam betas=(0.9, 0.999) and a constant learning rate of 1e-4 in our paper):\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0)\n",
    "\n",
    "print(x.shape, z.shape)\n"
   ],
   "id": "a26e8d5ddffec80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T15:33:41.645353Z",
     "start_time": "2024-06-03T15:33:39.794301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from collections import OrderedDict\n",
    "# from accelerate import Accelerator\n",
    "#\n",
    "# accelerator = Accelerator()\n",
    "# @torch.no_grad()\n",
    "# def update_ema(ema_model, model, decay=0.9999):\n",
    "#     \"\"\"\n",
    "#     Step the EMA model towards the current model.\n",
    "#     \"\"\"\n",
    "#     ema_params = OrderedDict(ema_model.named_parameters())\n",
    "#     model_params = OrderedDict(model.named_parameters())\n",
    "#\n",
    "#     for name, param in model_params.items():\n",
    "#         name = name.replace(\"module.\", \"\")\n",
    "#         # TODO: Consider applying only to params that require_grad to avoid small numerical changes of pos_embed\n",
    "#         ema_params[name].mul_(decay).add_(param.data, alpha=1 - decay)\n",
    "\n",
    "#\n",
    "# # Prepare models for training:\n",
    "# update_ema(ema, model, decay=0)  # Ensure EMA is initialized with synced weights\n",
    "# model.train()  # important! This enables embedding dropout for classifier-free guidance\n",
    "# ema.eval()  # EMA model should always be in eval mode\n",
    "# model, opt, loader = accelerator.prepare(model, opt, loader)\n",
    "print(x.shape, z.shape)\n",
    "\n",
    "# x = x.squeeze(dim=1)\n",
    "b = torch.zeros(x.shape)\n",
    "z = z.expand_as(b)\n",
    "print(z[0][0] == z[0][1])\n",
    "with torch.no_grad():\n",
    "    # Map input images to latent space + normalize latents:\n",
    "    x = vae.encode(x).latent_dist.sample().mul_(0.18215)\n",
    "    z = vae.encode(z).latent_dist.sample().mul_(0.18215)\n",
    "print(x.shape, z.shape)\n",
    "\n",
    "t = torch.randint(0, diffusion.num_timesteps, (z.shape[0],), device=Device)\n",
    "nx = model.forward(x, y, t, z)\n",
    "print(nx.shape)\n"
   ],
   "id": "272373334ac786a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 256, 256])\n",
      "tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        ...,\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]])\n",
      "torch.Size([1, 4, 32, 32]) torch.Size([1, 4, 32, 32])\n",
      "torch.Size([1, 4, 32, 32])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T15:33:43.007100Z",
     "start_time": "2024-06-03T15:33:41.646668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "print(nx.shape, nx.max())\n",
    "sample = vae.decode(nx / 0.18215).sample\n",
    "print(sample.shape)\n",
    "# Save and display images:\n",
    "save_image(sample, \"sample.png\", nrow=4, normalize=True, value_range=(0, 255))\n"
   ],
   "id": "23158849dfa1ddbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 32, 32]) tensor(0., grad_fn=<MaxBackward1>)\n",
      "torch.Size([1, 3, 256, 256])\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(sample\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Save and display images:\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m save_image(sample, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msample.png\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrow\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, normalize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, value_range\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m255\u001B[39m))\n",
      "File \u001B[0;32m~/miniconda3/envs/DiT/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/DiT/lib/python3.12/site-packages/torchvision/utils.py:151\u001B[0m, in \u001B[0;36msave_image\u001B[0;34m(tensor, fp, format, **kwargs)\u001B[0m\n\u001B[1;32m    149\u001B[0m ndarr \u001B[38;5;241m=\u001B[39m grid\u001B[38;5;241m.\u001B[39mmul(\u001B[38;5;241m255\u001B[39m)\u001B[38;5;241m.\u001B[39madd_(\u001B[38;5;241m0.5\u001B[39m)\u001B[38;5;241m.\u001B[39mclamp_(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m255\u001B[39m)\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m, torch\u001B[38;5;241m.\u001B[39muint8)\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m    150\u001B[0m im \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(ndarr)\n\u001B[0;32m--> 151\u001B[0m im\u001B[38;5;241m.\u001B[39msave(fp, \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mformat\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/DiT/lib/python3.12/site-packages/PIL/Image.py:2409\u001B[0m, in \u001B[0;36mImage.save\u001B[0;34m(self, fp, format, **params)\u001B[0m\n\u001B[1;32m   2407\u001B[0m open_fp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   2408\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_path(fp):\n\u001B[0;32m-> 2409\u001B[0m     filename \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mrealpath(os\u001B[38;5;241m.\u001B[39mfspath(fp))\n\u001B[1;32m   2410\u001B[0m     open_fp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2411\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m fp \u001B[38;5;241m==\u001B[39m sys\u001B[38;5;241m.\u001B[39mstdout:\n",
      "File \u001B[0;32m<frozen posixpath>:436\u001B[0m, in \u001B[0;36mrealpath\u001B[0;34m(filename, strict)\u001B[0m\n",
      "File \u001B[0;32m<frozen posixpath>:423\u001B[0m, in \u001B[0;36mabspath\u001B[0;34m(path)\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2c437a6aadfaa9b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1b0f445efb54d5b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "393dbedb8770429a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f2423dcae2c0f1ae",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
