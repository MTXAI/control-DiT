{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-04T02:38:42.582316Z",
     "start_time": "2024-06-04T02:38:42.575913Z"
    }
   },
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder, ImageNet\n",
    "\n",
    "from utils.dataset import CustomDataset\n",
    "from utils.image_tools import ImageTools\n",
    "\n",
    "# CUDA = True if torch.cuda.is_available() else False\n",
    "CUDA = False\n",
    "Tensor = torch.cuda.FloatTensor if CUDA else torch.FloatTensor\n",
    "Device = \"cuda\" if CUDA else \"cpu\"\n",
    "imgTools = ImageTools()\n",
    "real = True\n",
    "\n",
    "root = \"../datasets/tiny-imagenet-200_processed_train\"\n",
    "batch_size = 1\n",
    "num_workers = 0\n",
    "image_size = 256\n",
    "\n",
    "features_dir = os.path.join(root, f'imagenet{image_size}_features')\n",
    "labels_dir = os.path.join(root, f'imagenet{image_size}_labels')\n",
    "conditions_dir = os.path.join(root, f'imagenet{image_size}_conditions')\n",
    "\n",
    "# Setup data:\n",
    "dataset = CustomDataset(features_dir=features_dir, labels_dir=labels_dir, conditions_dir=conditions_dir)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    idx = 0\n",
    "    x, y, z = None, None, None\n",
    "    for x, y, z in loader:\n",
    "        idx += 1\n",
    "        x = x.to(Device)\n",
    "        y = y.to(Device)\n",
    "        z = z.to(Device)\n",
    "        print(idx, x.shape, y.shape, z.shape)\n",
    "        \n",
    "        x = x.squeeze(dim=1)\n",
    "        y = y.squeeze(dim=1)\n",
    "        z = z.squeeze(dim=1)\n",
    "        if x.shape[0] != batch_size:\n",
    "            print('break')\n",
    "            break\n",
    "        if idx > 0:\n",
    "            break\n",
    "\n",
    "    print(idx, x.shape, y.shape, z.shape)\n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "x, y, z = main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 torch.Size([2, 1, 4, 32, 32]) torch.Size([2, 1, 1]) torch.Size([2, 1, 4, 32, 32])\n",
      "break\n",
      "1 torch.Size([2, 4, 32, 32]) torch.Size([2, 1]) torch.Size([2, 4, 32, 32])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T19:29:05.591740Z",
     "start_time": "2024-06-03T19:28:55.919144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from diffusers import AutoencoderKL\n",
    "from copy import deepcopy\n",
    "from models.DiT import DiT_models\n",
    "from models.control_DiT import ControlDiT_models\n",
    "from diffusion import create_diffusion\n",
    "\n",
    "\n",
    "def requires_grad(model, flag=True):\n",
    "    \"\"\"\n",
    "    Set requires_grad flag for all parameters in a model.\n",
    "    \"\"\"\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "\n",
    "# Load model:\n",
    "assert image_size % 8 == 0, \"Image size must be divisible by 8 (for the VAE encoder).\"\n",
    "latent_size = image_size // 8\n",
    "model_type = 'DiT-XL/2'\n",
    "dit_model = DiT_models[model_type](\n",
    "    input_size=latent_size,\n",
    "    num_classes=1000,\n",
    ").to(Device)\n",
    "\n",
    "\n",
    "def find_model(model_name):\n",
    "    assert os.path.isfile(model_name), f'Could not find DiT checkpoint at {model_name}'\n",
    "    checkpoint = torch.load(model_name, map_location=lambda storage, loc: storage)\n",
    "    if \"ema\" in checkpoint:  # supports checkpoints from train.py\n",
    "        checkpoint = checkpoint[\"ema\"]\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "if not real:\n",
    "    ckpt_path = \"../pretrained_models/DiT-XL-2-256x256.pt\"\n",
    "    state_dict = find_model(ckpt_path)\n",
    "    dit_model.load_state_dict(state_dict)\n",
    "# dit_model.learn_sigma = False\n",
    "dit_model.eval()\n",
    "# model.eval()  # important!\n",
    "# Create model:\n",
    "model = ControlDiT_models[model_type](\n",
    "    dit=dit_model,\n",
    "    input_size=latent_size,\n",
    "    num_classes=1000,\n",
    "    learn_sigma=False,\n",
    ").to(Device)\n",
    "model.train()\n",
    "# # Note that parameter initialization is done within the DiT constructor\n",
    "# model = model.to(Device)\n",
    "# ema = deepcopy(model).to(Device)  # Create an EMA of the model for use after training\n",
    "# requires_grad(ema, False)\n",
    "diffusion = create_diffusion(timestep_respacing=\"\")  # default: 1000 steps, linear noise schedule\n",
    "if not real:\n",
    "    vae = AutoencoderKL.from_pretrained(f\"../pretrained_models/sd-vae-ft-ema\").to(Device)\n",
    "else:\n",
    "    vae = AutoencoderKL.from_pretrained(f\"stabilityai/sd-vae-ft-ema\").to(Device)\n",
    "# Setup optimizer (we used default Adam betas=(0.9, 0.999) and a constant learning rate of 1e-4 in our paper):\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0)\n"
   ],
   "id": "a26e8d5ddffec80",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T19:29:07.393409Z",
     "start_time": "2024-06-03T19:29:05.592325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from collections import OrderedDict\n",
    "# from accelerate import Accelerator\n",
    "#\n",
    "# accelerator = Accelerator()\n",
    "# @torch.no_grad()\n",
    "# def update_ema(ema_model, model, decay=0.9999):\n",
    "#     \"\"\"\n",
    "#     Step the EMA model towards the current model.\n",
    "#     \"\"\"\n",
    "#     ema_params = OrderedDict(ema_model.named_parameters())\n",
    "#     model_params = OrderedDict(model.named_parameters())\n",
    "#\n",
    "#     for name, param in model_params.items():\n",
    "#         name = name.replace(\"module.\", \"\")\n",
    "#         # TODO: Consider applying only to params that require_grad to avoid small numerical changes of pos_embed\n",
    "#         ema_params[name].mul_(decay).add_(param.data, alpha=1 - decay)\n",
    "\n",
    "#\n",
    "# # Prepare models for training:\n",
    "# update_ema(ema, model, decay=0)  # Ensure EMA is initialized with synced weights\n",
    "# model.train()  # important! This enables embedding dropout for classifier-free guidance\n",
    "# ema.eval()  # EMA model should always be in eval mode\n",
    "# model, opt, loader = accelerator.prepare(model, opt, loader)\n",
    "\n",
    "x = x.squeeze(dim=1)\n",
    "y = y.squeeze(dim=1)\n",
    "z = z.squeeze(dim=1)\n",
    "print(x.shape, y.shape, z.shape)\n",
    "t = torch.randint(0, diffusion.num_timesteps, (z.shape[0],), device=Device)\n",
    "nx = model.forward(x, y, t, z)\n",
    "print(nx.shape)\n",
    "\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "print(nx.shape, nx.max())\n",
    "sample = vae.decode(nx / 0.18215).sample\n",
    "print(sample.shape)\n",
    "# Save and display images:\n",
    "save_image(sample, \"sample.png\", nrow=4, normalize=True, value_range=(0, 255))\n"
   ],
   "id": "272373334ac786a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 32, 32]) torch.Size([1]) torch.Size([1, 4, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katz/miniconda3/envs/DiT/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 32, 32])\n",
      "torch.Size([1, 4, 32, 32]) tensor(0., grad_fn=<MaxBackward1>)\n",
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T19:29:07.395268Z",
     "start_time": "2024-06-03T19:29:07.394015Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "23158849dfa1ddbd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2c437a6aadfaa9b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T19:29:07.397709Z",
     "start_time": "2024-06-03T19:29:07.396499Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1b0f445efb54d5b1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T19:29:07.399511Z",
     "start_time": "2024-06-03T19:29:07.398310Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "393dbedb8770429a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T19:29:07.401081Z",
     "start_time": "2024-06-03T19:29:07.399940Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f2423dcae2c0f1ae",
   "outputs": [],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
