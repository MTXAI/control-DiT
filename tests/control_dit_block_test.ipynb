{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-03T07:46:09.157362Z",
     "start_time": "2024-06-03T07:46:07.729366Z"
    }
   },
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder, ImageNet\n",
    "\n",
    "from utils.dataset import CustomDataset\n",
    "from utils.image_tools import ImageTools\n",
    "\n",
    "# CUDA = True if torch.cuda.is_available() else False\n",
    "CUDA = False\n",
    "Tensor = torch.cuda.FloatTensor if CUDA else torch.FloatTensor\n",
    "Device = \"cuda\" if CUDA else \"cpu\"\n",
    "imgTools = ImageTools()\n",
    "\n",
    "root = \"../datasets/imagenet2012_processed_train\"\n",
    "batch_size = 1\n",
    "num_workers = 0\n",
    "image_size = 256\n",
    "\n",
    "features_dir = os.path.join(root, f'imagenet{image_size}_features')\n",
    "labels_dir = os.path.join(root, f'imagenet{image_size}_labels')\n",
    "conditions_dir = os.path.join(root, f'imagenet{image_size}_conditions')\n",
    "\n",
    "# Setup data:\n",
    "dataset = CustomDataset(features_dir=features_dir, labels_dir=labels_dir, conditions_dir=conditions_dir)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    idx = 0\n",
    "    x, y, z = None, None, None\n",
    "    for x, y, z in loader:\n",
    "        idx += 1\n",
    "        if idx < 11:\n",
    "            continue\n",
    "        x = x.to(Device)\n",
    "        y = y.to(Device)\n",
    "        z = z.to(Device)\n",
    "\n",
    "        if idx > 0:\n",
    "            break\n",
    "\n",
    "    print(idx, x.shape, y.shape, z.shape)\n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "x, y, z = main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 torch.Size([1, 1, 3, 256, 256]) torch.Size([1, 1]) torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T07:46:17.924020Z",
     "start_time": "2024-06-03T07:46:09.158403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from diffusers import AutoencoderKL\n",
    "from copy import deepcopy\n",
    "from models.DiT import DiT_models\n",
    "from models.control_DiT import ControlDiT_models\n",
    "from diffusion import create_diffusion\n",
    "\n",
    "\n",
    "def requires_grad(model, flag=True):\n",
    "    \"\"\"\n",
    "    Set requires_grad flag for all parameters in a model.\n",
    "    \"\"\"\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "\n",
    "# Load model:\n",
    "assert image_size % 8 == 0, \"Image size must be divisible by 8 (for the VAE encoder).\"\n",
    "latent_size = image_size // 8\n",
    "model_type = 'DiT-XL/2'\n",
    "dit_model = DiT_models[model_type](\n",
    "    input_size=latent_size,\n",
    "    num_classes=1000,\n",
    ").to(Device)\n",
    "\n",
    "\n",
    "def find_model(model_name):\n",
    "    assert os.path.isfile(model_name), f'Could not find DiT checkpoint at {model_name}'\n",
    "    checkpoint = torch.load(model_name, map_location=lambda storage, loc: storage)\n",
    "    if \"ema\" in checkpoint:  # supports checkpoints from train.py\n",
    "        checkpoint = checkpoint[\"ema\"]\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "ckpt_path = \"../pretrained_models/DiT-XL-2-256x256.pt\"\n",
    "state_dict = find_model(ckpt_path)\n",
    "dit_model.load_state_dict(state_dict)\n",
    "# dit_model.learn_sigma = False\n",
    "dit_model.eval()\n",
    "# model.eval()  # important!\n",
    "# Create model:\n",
    "model = ControlDiT_models[model_type](\n",
    "    dit=dit_model,\n",
    "    input_size=latent_size,\n",
    "    num_classes=1000,\n",
    "    learn_sigma=False,\n",
    ").to(Device)\n",
    "model.train()\n",
    "# # Note that parameter initialization is done within the DiT constructor\n",
    "# model = model.to(Device)\n",
    "# ema = deepcopy(model).to(Device)  # Create an EMA of the model for use after training\n",
    "# requires_grad(ema, False)\n",
    "diffusion = create_diffusion(timestep_respacing=\"\")  # default: 1000 steps, linear noise schedule\n",
    "vae = AutoencoderKL.from_pretrained(f\"../pretrained_models/sd-vae-ft-ema\").to(Device)\n",
    "# vae = AutoencoderKL.from_pretrained(f\"stabilityai/sd-vae-ft-ema\").to(Device)\n",
    "# Setup optimizer (we used default Adam betas=(0.9, 0.999) and a constant learning rate of 1e-4 in our paper):\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0)\n"
   ],
   "id": "a26e8d5ddffec80",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like ../pretrained_models/sd-vae-ft-ema is not the path to a directory containing a config.json file.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/diffusers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHFValidationError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/DiT/lib/python3.12/site-packages/diffusers/configuration_utils.py:383\u001B[0m, in \u001B[0;36mConfigMixin.load_config\u001B[0;34m(cls, pretrained_model_name_or_path, return_unused_kwargs, return_commit_hash, **kwargs)\u001B[0m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    382\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[0;32m--> 383\u001B[0m     config_file \u001B[38;5;241m=\u001B[39m hf_hub_download(\n\u001B[1;32m    384\u001B[0m         pretrained_model_name_or_path,\n\u001B[1;32m    385\u001B[0m         filename\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mconfig_name,\n\u001B[1;32m    386\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[1;32m    387\u001B[0m         force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[1;32m    388\u001B[0m         proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[1;32m    389\u001B[0m         resume_download\u001B[38;5;241m=\u001B[39mresume_download,\n\u001B[1;32m    390\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[1;32m    391\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[1;32m    392\u001B[0m         user_agent\u001B[38;5;241m=\u001B[39muser_agent,\n\u001B[1;32m    393\u001B[0m         subfolder\u001B[38;5;241m=\u001B[39msubfolder,\n\u001B[1;32m    394\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[1;32m    395\u001B[0m         local_dir\u001B[38;5;241m=\u001B[39mlocal_dir,\n\u001B[1;32m    396\u001B[0m         local_dir_use_symlinks\u001B[38;5;241m=\u001B[39mlocal_dir_use_symlinks,\n\u001B[1;32m    397\u001B[0m     )\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RepositoryNotFoundError:\n",
      "File \u001B[0;32m~/miniconda3/envs/DiT/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m arg_name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrepo_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 106\u001B[0m     validate_repo_id(arg_value)\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m arg_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m arg_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/DiT/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:154\u001B[0m, in \u001B[0;36mvalidate_repo_id\u001B[0;34m(repo_id)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m repo_id\u001B[38;5;241m.\u001B[39mcount(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 154\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\n\u001B[1;32m    155\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRepo id must be in the form \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrepo_name\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnamespace/repo_name\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    156\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. Use `repo_type` argument if needed.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    157\u001B[0m     )\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m REPO_ID_REGEX\u001B[38;5;241m.\u001B[39mmatch(repo_id):\n",
      "\u001B[0;31mHFValidationError\u001B[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../pretrained_models/sd-vae-ft-ema'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 53\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;66;03m# # Note that parameter initialization is done within the DiT constructor\u001B[39;00m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m# model = model.to(Device)\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;66;03m# ema = deepcopy(model).to(Device)  # Create an EMA of the model for use after training\u001B[39;00m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m# requires_grad(ema, False)\u001B[39;00m\n\u001B[1;32m     52\u001B[0m diffusion \u001B[38;5;241m=\u001B[39m create_diffusion(timestep_respacing\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# default: 1000 steps, linear noise schedule\u001B[39;00m\n\u001B[0;32m---> 53\u001B[0m vae \u001B[38;5;241m=\u001B[39m AutoencoderKL\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../pretrained_models/sd-vae-ft-ema\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mto(Device)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;66;03m# Setup optimizer (we used default Adam betas=(0.9, 0.999) and a constant learning rate of 1e-4 in our paper):\u001B[39;00m\n\u001B[1;32m     55\u001B[0m opt \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdamW(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-4\u001B[39m, weight_decay\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/DiT/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[1;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/DiT/lib/python3.12/site-packages/diffusers/models/modeling_utils.py:549\u001B[0m, in \u001B[0;36mModelMixin.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    542\u001B[0m user_agent \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    543\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdiffusers\u001B[39m\u001B[38;5;124m\"\u001B[39m: __version__,\n\u001B[1;32m    544\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    545\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mframework\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpytorch\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    546\u001B[0m }\n\u001B[1;32m    548\u001B[0m \u001B[38;5;66;03m# load config\u001B[39;00m\n\u001B[0;32m--> 549\u001B[0m config, unused_kwargs, commit_hash \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mload_config(\n\u001B[1;32m    550\u001B[0m     config_path,\n\u001B[1;32m    551\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[1;32m    552\u001B[0m     return_unused_kwargs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    553\u001B[0m     return_commit_hash\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    554\u001B[0m     force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[1;32m    555\u001B[0m     resume_download\u001B[38;5;241m=\u001B[39mresume_download,\n\u001B[1;32m    556\u001B[0m     proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[1;32m    557\u001B[0m     local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[1;32m    558\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[1;32m    559\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[1;32m    560\u001B[0m     subfolder\u001B[38;5;241m=\u001B[39msubfolder,\n\u001B[1;32m    561\u001B[0m     user_agent\u001B[38;5;241m=\u001B[39muser_agent,\n\u001B[1;32m    562\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    563\u001B[0m )\n\u001B[1;32m    565\u001B[0m \u001B[38;5;66;03m# load model\u001B[39;00m\n\u001B[1;32m    566\u001B[0m model_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/DiT/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[1;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/DiT/lib/python3.12/site-packages/diffusers/configuration_utils.py:420\u001B[0m, in \u001B[0;36mConfigMixin.load_config\u001B[0;34m(cls, pretrained_model_name_or_path, return_unused_kwargs, return_commit_hash, **kwargs)\u001B[0m\n\u001B[1;32m    415\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    416\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThere was a specific connection error when trying to load\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    417\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00merr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    418\u001B[0m     )\n\u001B[1;32m    419\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[0;32m--> 420\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    421\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe couldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt connect to \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m to load this model, couldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find it\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    422\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m in the cached files and it looks like \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not the path to a\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    423\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m directory containing a \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mconfig_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m file.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCheckout your internet connection or see how to\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    424\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m run the library in offline mode at\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    425\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/docs/diffusers/installation#offline-mode\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    426\u001B[0m     )\n\u001B[1;32m    427\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m:\n\u001B[1;32m    428\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    429\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt load config for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. If you were trying to load it from \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    430\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/models\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, make sure you don\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt have a local directory with the same name. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    431\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOtherwise, make sure \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is the correct path to a directory \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    432\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontaining a \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mconfig_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m file\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    433\u001B[0m     )\n",
      "\u001B[0;31mOSError\u001B[0m: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like ../pretrained_models/sd-vae-ft-ema is not the path to a directory containing a config.json file.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/diffusers/installation#offline-mode'."
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from collections import OrderedDict\n",
    "# from accelerate import Accelerator\n",
    "#\n",
    "# accelerator = Accelerator()\n",
    "# @torch.no_grad()\n",
    "# def update_ema(ema_model, model, decay=0.9999):\n",
    "#     \"\"\"\n",
    "#     Step the EMA model towards the current model.\n",
    "#     \"\"\"\n",
    "#     ema_params = OrderedDict(ema_model.named_parameters())\n",
    "#     model_params = OrderedDict(model.named_parameters())\n",
    "#\n",
    "#     for name, param in model_params.items():\n",
    "#         name = name.replace(\"module.\", \"\")\n",
    "#         # TODO: Consider applying only to params that require_grad to avoid small numerical changes of pos_embed\n",
    "#         ema_params[name].mul_(decay).add_(param.data, alpha=1 - decay)\n",
    "\n",
    "#\n",
    "# # Prepare models for training:\n",
    "# update_ema(ema, model, decay=0)  # Ensure EMA is initialized with synced weights\n",
    "# model.train()  # important! This enables embedding dropout for classifier-free guidance\n",
    "# ema.eval()  # EMA model should always be in eval mode\n",
    "# model, opt, loader = accelerator.prepare(model, opt, loader)\n",
    "\n",
    "x = x.squeeze(dim=1)\n",
    "b = torch.zeros(x.shape)\n",
    "z = z.expand_as(b)\n",
    "print(z[0][0] == z[0][1])\n",
    "# z = b\n",
    "with torch.no_grad():\n",
    "            # Map input images to latent space + normalize latents:\n",
    "    x = vae.encode(x).latent_dist.sample().mul_(0.18215)\n",
    "    z = vae.encode(z).latent_dist.sample().mul_(0.18215)\n",
    "y = y.squeeze(dim=1)\n",
    "z = z.squeeze(dim=1)\n",
    "t = torch.randint(0, diffusion.num_timesteps, (z.shape[0],), device=Device)\n",
    "nx = model.forward(x, y, t, z)\n",
    "print(nx.shape)\n"
   ],
   "id": "272373334ac786a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T07:46:17.925586Z",
     "start_time": "2024-06-03T07:46:17.925545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "print(nx.shape, nx.max())\n",
    "sample = vae.decode(nx / 0.18215).sample\n",
    "print(sample.shape)\n",
    "# Save and display images:\n",
    "save_image(sample, \"sample.png\", nrow=4, normalize=True, value_range=(0, 255))\n"
   ],
   "id": "23158849dfa1ddbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2c437a6aadfaa9b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1b0f445efb54d5b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "393dbedb8770429a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f2423dcae2c0f1ae",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
